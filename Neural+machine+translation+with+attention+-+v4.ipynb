{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "\n",
    "Build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\").Do this using an attention model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 8690.34it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('friday may 29 1970', '1970-05-29'),\n",
       " ('08 jun 2017', '2017-06-08'),\n",
       " ('25 february 2008', '2008-02-25'),\n",
       " ('friday october 21 1977', '1977-10-21'),\n",
       " ('15.05.14', '2014-05-15'),\n",
       " ('february 13 1976', '1976-02-13'),\n",
       " ('october 8 1984', '1984-10-08'),\n",
       " ('16 oct 1978', '1978-10-16'),\n",
       " ('november 23 1997', '1997-11-23'),\n",
       " ('sunday march 6 1988', '1988-03-06')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date)\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: friday may 29 1970\n",
      "Target date: 1970-05-29\n",
      "\n",
      "Source after preprocessing (indices): [18 28 21 16 13 34  0 24 13 34  0  5 12  0  4 12 10  3 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10  8  1  0  1  6  0  3 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Target after preprocessing (one-hot): [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural machine translation with attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') \n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers to implement `one_step_attention()`. In order to propagate a Keras tensor object X through one of these layers, use `layer(X)` (or `layer([X,Y])` if it requires multiple inputs.), e.g. `densor(X)` will propagate X through the `Dense(1)` layer defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a,s_prev])\n",
    "    e = densor1(concat)\n",
    "    energies = densor2(e)\n",
    "    alphas = activator(energies)\n",
    "    context = dotor([alphas,a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    for t in range(Ty):\n",
    "    \n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        out = output_layer(s)\n",
    "        \n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model([X, s0, c0], outputs)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional)  (None, 30, 64)        17920       input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)   (None, 30, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_5[0][0]                     \n",
      "                                                                   lstm_5[1][0]                     \n",
      "                                                                   lstm_5[2][0]                     \n",
      "                                                                   lstm_5[3][0]                     \n",
      "                                                                   lstm_5[4][0]                     \n",
      "                                                                   lstm_5[5][0]                     \n",
      "                                                                   lstm_5[6][0]                     \n",
      "                                                                   lstm_5[7][0]                     \n",
      "                                                                   lstm_5[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 30, 128)       0           bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[1][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[2][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[3][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[4][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[5][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[6][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[7][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[8][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[9][0]            \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   repeat_vector_2[10][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 30, 10)        1290        concatenate_2[1][0]              \n",
      "                                                                   concatenate_2[2][0]              \n",
      "                                                                   concatenate_2[3][0]              \n",
      "                                                                   concatenate_2[4][0]              \n",
      "                                                                   concatenate_2[5][0]              \n",
      "                                                                   concatenate_2[6][0]              \n",
      "                                                                   concatenate_2[7][0]              \n",
      "                                                                   concatenate_2[8][0]              \n",
      "                                                                   concatenate_2[9][0]              \n",
      "                                                                   concatenate_2[10][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 30, 1)         11          dense_4[1][0]                    \n",
      "                                                                   dense_4[2][0]                    \n",
      "                                                                   dense_4[3][0]                    \n",
      "                                                                   dense_4[4][0]                    \n",
      "                                                                   dense_4[5][0]                    \n",
      "                                                                   dense_4[6][0]                    \n",
      "                                                                   dense_4[7][0]                    \n",
      "                                                                   dense_4[8][0]                    \n",
      "                                                                   dense_4[9][0]                    \n",
      "                                                                   dense_4[10][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_5[1][0]                    \n",
      "                                                                   dense_5[2][0]                    \n",
      "                                                                   dense_5[3][0]                    \n",
      "                                                                   dense_5[4][0]                    \n",
      "                                                                   dense_5[5][0]                    \n",
      "                                                                   dense_5[6][0]                    \n",
      "                                                                   dense_5[7][0]                    \n",
      "                                                                   dense_5[8][0]                    \n",
      "                                                                   dense_5[9][0]                    \n",
      "                                                                   dense_5[10][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1, 64)         0           attention_weights[1][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "                                                                   attention_weights[10][0]         \n",
      "                                                                   bidirectional_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    [(None, 64), (None, 6 33024       dot_2[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_2[1][0]                      \n",
      "                                                                   lstm_5[0][0]                     \n",
      "                                                                   lstm_5[0][2]                     \n",
      "                                                                   dot_2[2][0]                      \n",
      "                                                                   lstm_5[1][0]                     \n",
      "                                                                   lstm_5[1][2]                     \n",
      "                                                                   dot_2[3][0]                      \n",
      "                                                                   lstm_5[2][0]                     \n",
      "                                                                   lstm_5[2][2]                     \n",
      "                                                                   dot_2[4][0]                      \n",
      "                                                                   lstm_5[3][0]                     \n",
      "                                                                   lstm_5[3][2]                     \n",
      "                                                                   dot_2[5][0]                      \n",
      "                                                                   lstm_5[4][0]                     \n",
      "                                                                   lstm_5[4][2]                     \n",
      "                                                                   dot_2[6][0]                      \n",
      "                                                                   lstm_5[5][0]                     \n",
      "                                                                   lstm_5[5][2]                     \n",
      "                                                                   dot_2[7][0]                      \n",
      "                                                                   lstm_5[6][0]                     \n",
      "                                                                   lstm_5[6][2]                     \n",
      "                                                                   dot_2[8][0]                      \n",
      "                                                                   lstm_5[7][0]                     \n",
      "                                                                   lstm_5[7][2]                     \n",
      "                                                                   dot_2[9][0]                      \n",
      "                                                                   lstm_5[8][0]                     \n",
      "                                                                   lstm_5[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 11)            715         lstm_5[0][0]                     \n",
      "                                                                   lstm_5[1][0]                     \n",
      "                                                                   lstm_5[2][0]                     \n",
      "                                                                   lstm_5[3][0]                     \n",
      "                                                                   lstm_5[4][0]                     \n",
      "                                                                   lstm_5[5][0]                     \n",
      "                                                                   lstm_5[6][0]                     \n",
      "                                                                   lstm_5[7][0]                     \n",
      "                                                                   lstm_5[8][0]                     \n",
      "                                                                   lstm_5[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 30s - loss: 16.1261 - dense_7_loss_1: 1.1973 - dense_7_loss_2: 0.9837 - dense_7_loss_3: 1.7017 - dense_7_loss_4: 2.6753 - dense_7_loss_5: 0.7156 - dense_7_loss_6: 1.1886 - dense_7_loss_7: 2.5740 - dense_7_loss_8: 0.8522 - dense_7_loss_9: 1.6491 - dense_7_loss_10: 2.5886 - dense_7_acc_1: 0.5396 - dense_7_acc_2: 0.7061 - dense_7_acc_3: 0.3225 - dense_7_acc_4: 0.0740 - dense_7_acc_5: 0.9649 - dense_7_acc_6: 0.4050 - dense_7_acc_7: 0.0692 - dense_7_acc_8: 0.9545 - dense_7_acc_9: 0.2429 - dense_7_acc_10: 0.1036    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea04e4d320>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "source: 5 April 09\n",
      "output: 2009-05-05\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-21\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
